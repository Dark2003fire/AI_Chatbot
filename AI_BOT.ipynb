{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf15985c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chatterbot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchatterbot\u001b[39;00m  \u001b[38;5;28;01mimport\u001b[39;00m ChatBot\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chatterbot'"
     ]
    }
   ],
   "source": [
    "from chatterbot  import ChatBot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0b2acdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;241m*\u001b[39mrnames):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m), \u001b[38;5;241m*\u001b[39mrnames))\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      9\u001b[0m setup(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyutilib.subprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.5.4\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     11\u001b[0m     maintainer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWilliam E. Hart\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m     maintainer_email\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwehart@sandia.gov\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://software.sandia.gov/svn/public/pyutilib/pyutilib.subprocess\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m     license \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBSD\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m     platforms \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     16\u001b[0m     description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPyUtilib utilites for managing subprocesses.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m---> 17\u001b[0m     long_description \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mREADME.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     18\u001b[0m     classifiers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDevelopment Status :: 4 - Beta\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIntended Audience :: End Users/Desktop\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLicense :: OSI Approved :: BSD License\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNatural Language :: English\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOperating System :: Microsoft :: Windows\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOperating System :: Unix\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProgramming Language :: Python\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProgramming Language :: Unix Shell\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTopic :: Scientific/Engineering :: Mathematics\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTopic :: Software Development :: Libraries :: Python Modules\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     29\u001b[0m       packages\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyutilib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyutilib.subprocess\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyutilib.subprocess.tests\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     30\u001b[0m       keywords\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutility\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     31\u001b[0m       namespace_packages\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyutilib\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     32\u001b[0m       install_requires\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyutilib.common\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyutilib.services\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     33\u001b[0m       )\n",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36mread\u001b[1;34m(*rnames)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;241m*\u001b[39mrnames):\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18;43m__file__\u001b[39;49m), \u001b[38;5;241m*\u001b[39mrnames))\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from setuptools import setup\n",
    "\n",
    "\n",
    "def read(*rnames):\n",
    "    return open(os.path.join(os.path.dirname(__file__), *rnames)).read()\n",
    "\n",
    "\n",
    "setup(name=\"pyutilib.subprocess\",\n",
    "    version='3.5.4',\n",
    "    maintainer='William E. Hart',\n",
    "    maintainer_email='wehart@sandia.gov',\n",
    "    url = 'https://software.sandia.gov/svn/public/pyutilib/pyutilib.subprocess',\n",
    "    license = 'BSD',\n",
    "    platforms = [\"any\"],\n",
    "    description = 'PyUtilib utilites for managing subprocesses.',\n",
    "    long_description = read('README.txt'),\n",
    "    classifiers = [\n",
    "        'Development Status :: 4 - Beta',\n",
    "        'Intended Audience :: End Users/Desktop',\n",
    "        'License :: OSI Approved :: BSD License',\n",
    "        'Natural Language :: English',\n",
    "        'Operating System :: Microsoft :: Windows',\n",
    "        'Operating System :: Unix',\n",
    "        'Programming Language :: Python',\n",
    "        'Programming Language :: Unix Shell',\n",
    "        'Topic :: Scientific/Engineering :: Mathematics',\n",
    "        'Topic :: Software Development :: Libraries :: Python Modules'],\n",
    "      packages=['pyutilib', 'pyutilib.subprocess', 'pyutilib.subprocess.tests'],\n",
    "      keywords=['utility'],\n",
    "      namespace_packages=['pyutilib'],\n",
    "      install_requires=['pyutilib.common', 'pyutilib.services']\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0afc655d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou are tying to install ChatterBot on Python version \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease install ChatterBot in Python 3 instead.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     12\u001b[0m             platform\u001b[38;5;241m.\u001b[39mpython_version()\n\u001b[0;32m     13\u001b[0m         )\n\u001b[0;32m     14\u001b[0m     )\n\u001b[0;32m     16\u001b[0m config \u001b[38;5;241m=\u001b[39m configparser\u001b[38;5;241m.\u001b[39mConfigParser()\n\u001b[1;32m---> 18\u001b[0m current_directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m))\n\u001b[0;32m     19\u001b[0m config_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(current_directory, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msetup.cfg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m config\u001b[38;5;241m.\u001b[39mread(config_file_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import configparser\n",
    "from setuptools import setup\n",
    "\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\n",
    "        'You are tying to install ChatterBot on Python version {}.\\n'\n",
    "        'Please install ChatterBot in Python 3 instead.'.format(\n",
    "            platform.python_version()\n",
    "        )\n",
    "    )\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "current_directory = os.path.dirname(os.path.abspath(__file__))\n",
    "config_file_path = os.path.join(current_directory, 'setup.cfg')\n",
    "\n",
    "config.read(config_file_path)\n",
    "\n",
    "VERSION = config['chatterbot']['version']\n",
    "AUTHOR = config['chatterbot']['author']\n",
    "AUTHOR_EMAIL = config['chatterbot']['email']\n",
    "URL = config['chatterbot']['url']\n",
    "\n",
    "with open('README.md') as f:\n",
    "    LONG_DESCRIPTION = f.read()\n",
    "\n",
    "REQUIREMENTS = []\n",
    "DEPENDENCIES = []\n",
    "\n",
    "with open('requirements.txt') as requirements:\n",
    "    for requirement in requirements.readlines():\n",
    "        if requirement.startswith('git+git://'):\n",
    "            DEPENDENCIES.append(requirement)\n",
    "        else:\n",
    "            REQUIREMENTS.append(requirement)\n",
    "\n",
    "\n",
    "setup(\n",
    "    name='ChatterBot',\n",
    "    version=VERSION,\n",
    "    url=URL,\n",
    "    download_url='{}/tarball/{}'.format(URL, VERSION),\n",
    "    project_urls={\n",
    "        'Documentation': 'https://chatterbot.readthedocs.io',\n",
    "    },\n",
    "    description='ChatterBot is a machine learning, conversational dialog engine.',\n",
    "    long_description=LONG_DESCRIPTION,\n",
    "    long_description_content_type='text/markdown',\n",
    "    author=AUTHOR,\n",
    "    author_email=AUTHOR_EMAIL,\n",
    "    packages=[\n",
    "        'chatterbot',\n",
    "        'chatterbot.storage',\n",
    "        'chatterbot.logic',\n",
    "        'chatterbot.ext',\n",
    "        'chatterbot.ext.sqlalchemy_app',\n",
    "        'chatterbot.ext.django_chatterbot',\n",
    "        'chatterbot.ext.django_chatterbot.migrations',\n",
    "    ],\n",
    "    package_dir={'chatterbot': 'chatterbot'},\n",
    "    include_package_data=True,\n",
    "    install_requires=REQUIREMENTS,\n",
    "    dependency_links=DEPENDENCIES,\n",
    "    python_requires='>=3.4, <=3.8',\n",
    "    license='BSD',\n",
    "    zip_safe=True,\n",
    "    platforms=['any'],\n",
    "    keywords=['ChatterBot', 'chatbot', 'chat', 'bot'],\n",
    "    classifiers=[\n",
    "        'Development Status :: 4 - Beta',\n",
    "        'Intended Audience :: Developers',\n",
    "        'License :: OSI Approved :: BSD License',\n",
    "        'Environment :: Console',\n",
    "        'Environment :: Web Environment',\n",
    "        'Operating System :: OS Independent',\n",
    "        'Topic :: Software Development :: Libraries :: Python Modules',\n",
    "        'Topic :: Communications :: Chat',\n",
    "        'Topic :: Internet',\n",
    "        'Programming Language :: Python',\n",
    "        'Programming Language :: Python :: 3',\n",
    "        'Programming Language :: Python :: 3.4',\n",
    "        'Programming Language :: Python :: 3.5',\n",
    "        'Programming Language :: Python :: 3.6',\n",
    "        'Programming Language :: Python :: 3.7',\n",
    "        'Programming Language :: Python :: 3.8',\n",
    "        'Programming Language :: Python :: 3 :: Only',\n",
    "    ],\n",
    "    test_suite='tests'\n",
    ")\n",
    "\n",
    "print(os.getcwd())\n",
    "print(sys.argv[0])\n",
    "print(os.path.dirname(os.path.realpath('__file__')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22fab535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hjha0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b78bbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8989772d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "corpus =  PlaintextCorpusReader('ChatBot.txt' ,'r')\n",
    "print(corpus.fileids())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "792b4463",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "concat() expects at least one object!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal Sentnces :\u001b[39m\u001b[38;5;124m\"\u001b[39m , \u001b[38;5;28mlen\u001b[39m(sentences))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst Sentences:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sentences[\u001b[38;5;241m0\u001b[39m])\n",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36mPlaintextCorpusReader.sents\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sent_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo sentence tokenizer for this corpus\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCorpusView\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_sent_block\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfileid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabspaths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\util.py:444\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(docs)\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m docs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(docs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcat() expects at least one object!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    446\u001b[0m types \u001b[38;5;241m=\u001b[39m {d\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m docs}\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# If they're all strings, use string concatenation.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: concat() expects at least one object!"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sentences = corpus.sents()\n",
    "print(\"total Sentnces :\" , len(sentences))\n",
    "print(\"First Sentences:\", sentences[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80ca8dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "from nltk.corpus.reader.api import *\n",
    "from nltk.corpus.reader.util import *\n",
    "from nltk.tokenize import *\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "class PlaintextCorpusReader(CorpusReader):\n",
    "    \"\"\"\n",
    "    Reader for corpora that consist of plaintext documents.  Paragraphs\n",
    "    are assumed to be split using blank lines.  Sentences and words can\n",
    "    be tokenized using the default tokenizers, or by custom tokenizers\n",
    "    specified as parameters to the constructor.\n",
    "\n",
    "    This corpus reader can be customized (e.g., to skip preface\n",
    "    sections of specific document formats) by creating a subclass and\n",
    "    overriding the ``CorpusView`` class variable.\n",
    "    \"\"\"\n",
    "\n",
    "    CorpusView = StreamBackedCorpusView\n",
    "    \"\"\"The corpus view class used by this reader.  Subclasses of\n",
    "       ``PlaintextCorpusReader`` may specify alternative corpus view\n",
    "       classes (e.g., to skip the preface sections of documents.)\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        fileids,\n",
    "        word_tokenizer=WordPunctTokenizer(),\n",
    "        sent_tokenizer=nltk.data.LazyLoader(\"tokenizers/punkt/english.pickle\"),\n",
    "        para_block_reader=read_blankline_block,\n",
    "        encoding=\"utf8\",\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Construct a new plaintext corpus reader for a set of documents\n",
    "        located at the given root directory.  Example usage:\n",
    "\n",
    "            >>> root = '/usr/local/share/nltk_data/corpora/webtext/'\n",
    "            >>> reader = PlaintextCorpusReader(root, '.*\\.txt') # doctest: +SKIP\n",
    "\n",
    "        :param root: The root directory for this corpus.\n",
    "        :param fileids: A list or regexp specifying the fileids in this corpus.\n",
    "        :param word_tokenizer: Tokenizer for breaking sentences or\n",
    "            paragraphs into words.\n",
    "        :param sent_tokenizer: Tokenizer for breaking paragraphs\n",
    "            into words.\n",
    "        :param para_block_reader: The block reader used to divide the\n",
    "            corpus into paragraph blocks.\n",
    "        \"\"\"\n",
    "        CorpusReader.__init__(self, root, fileids, encoding)\n",
    "        self._word_tokenizer = word_tokenizer\n",
    "        self._sent_tokenizer = sent_tokenizer\n",
    "        self._para_block_reader = para_block_reader\n",
    "\n",
    "\n",
    "    def words(self, fileids=None):\n",
    "        \"\"\"\n",
    "        :return: the given file(s) as a list of words\n",
    "            and punctuation symbols.\n",
    "        :rtype: list(str)\n",
    "        \"\"\"\n",
    "        return concat(\n",
    "            [\n",
    "                self.CorpusView(path, self._read_word_block, encoding=enc)\n",
    "                for (path, enc, fileid) in self.abspaths(fileids, True, True)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "    def sents(self, fileids=None):\n",
    "        \"\"\"\n",
    "        :return: the given file(s) as a list of\n",
    "            sentences or utterances, each encoded as a list of word\n",
    "            strings.\n",
    "        :rtype: list(list(str))\n",
    "        \"\"\"\n",
    "        if self._sent_tokenizer is None:\n",
    "            raise ValueError(\"No sentence tokenizer for this corpus\")\n",
    "\n",
    "        return concat(\n",
    "            [\n",
    "                self.CorpusView(path, self._read_sent_block, encoding=enc)\n",
    "                for (path, enc, fileid) in self.abspaths(fileids, True, True)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "    def paras(self, fileids=None):\n",
    "        \"\"\"\n",
    "        :return: the given file(s) as a list of\n",
    "            paragraphs, each encoded as a list of sentences, which are\n",
    "            in turn encoded as lists of word strings.\n",
    "        :rtype: list(list(list(str)))\n",
    "        \"\"\"\n",
    "        if self._sent_tokenizer is None:\n",
    "            raise ValueError(\"No sentence tokenizer for this corpus\")\n",
    "\n",
    "        return concat(\n",
    "            [\n",
    "                self.CorpusView(path, self._read_para_block, encoding=enc)\n",
    "                for (path, enc, fileid) in self.abspaths(fileids, True, True)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "    def _read_word_block(self, stream):\n",
    "        words = []\n",
    "        for i in range(20):  # Read 20 lines at a time.\n",
    "            words.extend(self._word_tokenizer.tokenize(stream.readline()))\n",
    "        return words\n",
    "\n",
    "    def _read_sent_block(self, stream):\n",
    "        sents = []\n",
    "        for para in self._para_block_reader(stream):\n",
    "            sents.extend(\n",
    "                [\n",
    "                    self._word_tokenizer.tokenize(sent)\n",
    "                    for sent in self._sent_tokenizer.tokenize(para)\n",
    "                ]\n",
    "            )\n",
    "        return sents\n",
    "\n",
    "    def _read_para_block(self, stream):\n",
    "        paras = []\n",
    "        for para in self._para_block_reader(stream):\n",
    "            paras.append(\n",
    "                [\n",
    "                    self._word_tokenizer.tokenize(sent)\n",
    "                    for sent in self._sent_tokenizer.tokenize(para)\n",
    "                ]\n",
    "            )\n",
    "        return paras\n",
    "\n",
    "\n",
    "\n",
    "class CategorizedPlaintextCorpusReader(CategorizedCorpusReader, PlaintextCorpusReader):\n",
    "    \"\"\"\n",
    "    A reader for plaintext corpora whose documents are divided into\n",
    "    categories based on their file identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader.  Categorization arguments\n",
    "        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
    "        the ``CategorizedCorpusReader`` constructor.  The remaining arguments\n",
    "        are passed to the ``PlaintextCorpusReader`` constructor.\n",
    "        \"\"\"\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        PlaintextCorpusReader.__init__(self, *args, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "# FIXME: Is there a better way? How to not hardcode this?\n",
    "#       Possibly, add a language kwargs to CategorizedPlaintextCorpusReader to\n",
    "#       override the `sent_tokenizer`.\n",
    "class PortugueseCategorizedPlaintextCorpusReader(CategorizedPlaintextCorpusReader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        kwargs[\"sent_tokenizer\"] = nltk.data.LazyLoader(\n",
    "            \"tokenizers/punkt/portuguese.pickle\"\n",
    "        )\n",
    "        PlaintextCorpusReader.__init__(self, *args, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "class EuroparlCorpusReader(PlaintextCorpusReader):\n",
    "\n",
    "    \"\"\"\n",
    "    Reader for Europarl corpora that consist of plaintext documents.\n",
    "    Documents are divided into chapters instead of paragraphs as\n",
    "    for regular plaintext documents. Chapters are separated using blank\n",
    "    lines. Everything is inherited from ``PlaintextCorpusReader`` except\n",
    "    that:\n",
    "\n",
    "    - Since the corpus is pre-processed and pre-tokenized, the\n",
    "      word tokenizer should just split the line at whitespaces.\n",
    "    - For the same reason, the sentence tokenizer should just\n",
    "      split the paragraph at line breaks.\n",
    "    - There is a new 'chapters()' method that returns chapters instead\n",
    "      instead of paragraphs.\n",
    "    - The 'paras()' method inherited from PlaintextCorpusReader is\n",
    "      made non-functional to remove any confusion between chapters\n",
    "      and paragraphs for Europarl.\n",
    "    \"\"\"\n",
    "\n",
    "    def _read_word_block(self, stream):\n",
    "        words = []\n",
    "        for i in range(20):  # Read 20 lines at a time.\n",
    "            words.extend(stream.readline().split())\n",
    "        return words\n",
    "\n",
    "    def _read_sent_block(self, stream):\n",
    "        sents = []\n",
    "        for para in self._para_block_reader(stream):\n",
    "            sents.extend([sent.split() for sent in para.splitlines()])\n",
    "        return sents\n",
    "\n",
    "    def _read_para_block(self, stream):\n",
    "        paras = []\n",
    "        for para in self._para_block_reader(stream):\n",
    "            paras.append([sent.split() for sent in para.splitlines()])\n",
    "        return paras\n",
    "\n",
    "    def chapters(self, fileids=None):\n",
    "        \"\"\"\n",
    "        :return: the given file(s) as a list of\n",
    "            chapters, each encoded as a list of sentences, which are\n",
    "            in turn encoded as lists of word strings.\n",
    "        :rtype: list(list(list(str)))\n",
    "        \"\"\"\n",
    "        return concat(\n",
    "            [\n",
    "                self.CorpusView(fileid, self._read_para_block, encoding=enc)\n",
    "                for (fileid, enc) in self.abspaths(fileids, True)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "    def paras(self, fileids=None):\n",
    "        raise NotImplementedError(\n",
    "            \"The Europarl corpus reader does not support paragraphs. Please use chapters() instead.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9de5303e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c57949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1243525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
